# -*- coding: utf-8 -*-
"""EduTech_DS_03.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q_TJyyrEoiWxxa1Cj9bjZV_yzpJ2UWxb
"""

import pandas as pd
import numpy as np
import seaborn as sns
from matplotlib import pyplot as plt
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
isp4=pd.read_csv("/content/Untitled Folder/imployment.csv")

from sklearn.preprocessing import LabelEncoder

le=LabelEncoder()

isp4['Gender'] = le.fit_transform(isp4['Gender'])
isp4['MaritalStatus'] = le.fit_transform(isp4['MaritalStatus'])
isp4['Education'] = le.fit_transform(isp4['Education'])
isp4['EnvironmentSatisfaction'] = le.fit_transform(isp4['EnvironmentSatisfaction'])
isp4['OverTime'] = le.fit_transform(isp4['OverTime'])

isp4_numeric=isp4.select_dtypes(int)

isp4_numeric.head()

isp4.info()

# cleaning the dataset from outliers
for i in isp4_numeric:
    sns.boxplot(isp4_numeric[i])
    plt.show()

# removing outlier from monthly billing
Q1 = isp4_numeric['MonthlyIncome'].quantile(0.25)
Q3 = isp4_numeric['MonthlyIncome'].quantile(0.75)

Q1,Q3
IQR=Q3-Q1
print('IQR is',IQR)

low_lim = Q1 - 1.5 * IQR
up_lim = Q3 + 1.5 * IQR
print('low_limit is', low_lim)
print('up_limit is', up_lim)

# outlier =[]
# for x in isp4_numeric['MonthlyBilling']:
#     if ((x> up_lim) or (x<low_lim)):
#         outlier.append(x)
# print('outlier in the dataset is', outlier)

isp4_numeric=isp4_numeric[(isp4_numeric['TrainingTimesLastYear'] < up_lim) & (isp4_numeric['TrainingTimesLastYear'] > low_lim)]

# removing outlier from years at company
Q1 = isp4_numeric['YearsAtCompany'].quantile(0.25)
Q3 = isp4_numeric['YearsAtCompany'].quantile(0.75)

Q1,Q3
IQR=Q3-Q1
print('IQR is',IQR)

low_lim = Q1 - 1.5 * IQR
up_lim = Q3 + 1.5 * IQR
print('low_limit is', low_lim)
print('up_limit is', up_lim)


# outlier =[]
# for x in new_df_numric['YearsAtCompany']:
#     if ((x> up_lim) or (x<low_lim)):
#         outlier.append(x)
# print('outlier in the dataset is', outlier)

isp4_numeric=isp4_numeric[(isp4_numeric['YearsAtCompany'] < up_lim) & (isp4_numeric['YearsAtCompany'] > low_lim)]

# removing outlier from years since last promotion
Q1 = isp4_numeric['YearsSinceLastPromotion'].quantile(0.25)
Q3 = isp4_numeric['YearsSinceLastPromotion'].quantile(0.75)

Q1,Q3
IQR=Q3-Q1
print('IQR is',IQR)

low_lim = Q1 - 1.5 * IQR
up_lim = Q3 + 1.5 * IQR
print('low_limit is', low_lim)
print('up_limit is', up_lim)


# outlier =[]
# for x in new_df_numric['YearsSinceLastPromotion']:
#     if ((x> up_lim) or (x<low_lim)):
#         outlier.append(x)
# print('outlier in the dataset is', outlier)

isp4_numeric=isp4_numeric[(isp4_numeric['YearsSinceLastPromotion'] < up_lim) & (isp4_numeric['YearsSinceLastPromotion'] > low_lim)]

Q1 = isp4_numeric['YearsWithCurrManager'].quantile(0.25)
Q3 = isp4_numeric['YearsWithCurrManager'].quantile(0.75)

Q1,Q3
IQR=Q3-Q1
print('IQR is',IQR)

low_lim = Q1 - 1.5 * IQR
up_lim = Q3 + 1.5 * IQR
print('low_limit is', low_lim)
print('up_limit is', up_lim)

# outliers = []
# for i in isp4_numeric['YearsWithCurrManager ']:
#     if ((i < low_lim) or (i > up_lim)):
#         outliers.append(i)
# print(outliers)

isp4_numeric = isp4_numeric[(isp4_numeric['YearsWithCurrManager'] < up_lim) & (isp4_numeric['YearsWithCurrManager'] > low_lim)]
isp4_numeric.shape

isp4_numeric=isp4_numeric.reset_index()

isp4_numeric.drop(['index'],axis=1,inplace=True)

isp4_numeric.drop(['Gender','HourlyRate'],axis=1,inplace=True)

isp4_numeric.shape

from sklearn.model_selection import train_test_split

isp4_numeric.columns

# x=isp4_numeric[['EmployeId', 'Age', 'Gender', 'MaritalStatus', 'Travelling',
#        'Qualifications', 'EmployeSatisfaction', 'JobEngagement', 'JobLevel',
#        'JobSatisfaction', 'DailyBilling', 'HourBilling', 'MonthlyBilling',
#        'MonthlyRate', 'Work Experience', 'OverTime', 'PercentSalaryHike',
#        'Last Rating', 'RelationshipSatisfaction', 'Hours', 'StockOptionLevel',
#        'TrainingTimesLastYear', 'Work&Life', 'YearsAtCompany',
#        'YearsInCurrentRole', 'YearsSinceLastPromotion',
#        'YearsWithCurrentManager', 'DistanceFromHome']].values

x = isp4_numeric.drop(['Education'],axis=1)
y = isp4_numeric['Education']

xtrain , xtest , ytrain , ytest = train_test_split(x,y,test_size = 0.3, random_state = 42)

from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(n_estimators=50)

rf_model.fit(xtrain, ytrain)

rf_model.score(xtest,ytest)

from sklearn.metrics import accuracy_score

pred=rf_model.predict(xtest)

score = accuracy_score(ytest,pred)
score

from sklearn.ensemble import ExtraTreesClassifier

etc=ExtraTreesClassifier(n_estimators=50)

etc.fit(xtrain,ytrain)

etc.score(xtest,ytest)

pred=etc.predict(xtest)

score = accuracy_score(ytest,pred)
score

from sklearn.ensemble import GradientBoostingClassifier

gbc = GradientBoostingClassifier()

gbc.fit(xtrain,ytrain)

gbc.score(xtest,ytest)

pred=gbc.predict(xtest)

score = accuracy_score(ytest,pred)
score

confusion_matrix(ytest,pred)

# confusion matrix metrics
matrix=classification_report(ytest,pred)

print(matrix)

# Best model will be Rnadom Forest Classifier model.
# As its accuracy is highest.